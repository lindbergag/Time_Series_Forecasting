labs(y = NULL) +
ggtitle("Occurences of Symbols")
ggsave('symbols200-300.png')
#Visual Most Symbols
tweets %>%
count(symbols, sort = TRUE) %>%
filter(!(is.na(symbols))) %>%
filter(n > 150) %>%
filter(n < 201) %>%
mutate(word = reorder(symbols, n)) %>%
ggplot(aes(n, word)) +
geom_col() +
labs(y = NULL) +
ggtitle("Occurences of Symbols")
ggsave('symbols150-200.png')
#Visual Most Symbols
tweets %>%
count(symbols, sort = TRUE) %>%
filter(!(is.na(symbols))) %>%
filter(n > 100) %>%
filter(n < 151) %>%
mutate(word = reorder(symbols, n)) %>%
ggplot(aes(n, word)) +
geom_col() +
labs(y = NULL) +
ggtitle("Occurences of Symbols")
ggsave('symbols100-150.png')
#Visual Most Symbols
tweets %>%
count(symbols, sort = TRUE) %>%
filter(!(is.na(symbols))) %>%
filter(n > 75) %>%
filter(n < 101) %>%
mutate(word = reorder(symbols, n)) %>%
ggplot(aes(n, word)) +
geom_col() +
labs(y = NULL) +
ggtitle("Occurences of Symbols")
ggsave('symbols75-100.png')
#Visual Most Symbols
tweets %>%
count(symbols, sort = TRUE) %>%
filter(!(is.na(symbols))) %>%
filter(n > 65) %>%
filter(n < 76) %>%
mutate(word = reorder(symbols, n)) %>%
ggplot(aes(n, word)) +
geom_col() +
labs(y = NULL) +
ggtitle("Occurences of Symbols")
ggsave('symbols65-75.png')
#Visual Most Symbols
tweets %>%
count(symbols, sort = TRUE) %>%
filter(!(is.na(symbols))) %>%
filter(n > 55) %>%
filter(n < 66) %>%
mutate(word = reorder(symbols, n)) %>%
ggplot(aes(n, word)) +
geom_col() +
labs(y = NULL) +
ggtitle("Occurences of Symbols")
ggsave('symbols55-65.png')
#Visual Most Symbols
tweets %>%
count(symbols, sort = TRUE) %>%
filter(!(is.na(symbols))) %>%
filter(n > 50) %>%
filter(n < 56) %>%
mutate(word = reorder(symbols, n)) %>%
ggplot(aes(n, word)) +
geom_col() +
labs(y = NULL) +
ggtitle("Occurences of Symbols")
ggsave('symbols50-55.png')
tokens = c('boo', 'spirit', 'treeb', 'lqdr', 'comb', 'beets', 'hec', 'power', 'tarot',
'oxd', 'woo', 'brush', 'wigo', 'atlas', 'soul', 'dfi', 'weve', 'geist', 'spa')
tweets %>%
count(symbols, sort = TRUE) %>%
filter(symbols %in% tokens)
tweet.counts <- tweets %>%
count(symbols, sort = TRUE) %>%
filter(symbols %in% tokens)
mean(tweet.counts$n)
mad(tweet.counts$n)
library(devtools)
install.packages('devtools')
library(devtools)
devtools::install_github('viking/r-yaml')
pkgbuild::check_build_tools(debug = TRUE)
pkgbuild::check_build_tools(debug = TRUE)
install.packages("installr")
libary(installr)
library('installr')
updateR()
library(devtools)
devtools::install_github('viking/r-yaml')
library(devtools)
devtools::install_github('viking/r-yaml')
remove.packages("yaml")
knitr::opts_chunk$set(echo = TRUE,
cache = FALSE)
if (!require("tidyverse")) install.packages("tidyverse")
library(tidyverse)
if (!require("tm")) install.packages("tm")
library(tm)
if (!require("ggplot2")) install.packages("ggplot2")
library(ggplot2)
if (!require("gridExtra")) install.packages("gridExtra")
library(gridExtra)
if (!require("lubridate")) install.packages("lubridate")
library(lubridate)
if (!require("xts")) install.packages("xts")
library(xts)
if (!require("zoo")) install.packages("zoo")
library(zoo)
if (!require("fable")) install.packages("fable")
library(fable)
if (!require("SnowballC")) install.packages("SnowballC")
library(forecast)
if (!require("anytime")) install.packages("anytime")
library(anytime)
# read in csv file of tweets
rawish.data <- read.csv('influencerdata_clean.csv')
tweets <- rawish.data
tweets$created_at <- as_datetime(tweets$created_at)
tweets$quoted_created_at <- as_datetime(tweets$quoted_created_at, tz = "UTC")
tweets$retweet_created_at <- as_datetime(tweets$retweet_created_at, tz = "UTC")
tweets$account_created_at <- as_datetime(tweets$account_created_at, tz = "UTC")
# common artifacts that remain after cleaning
other.words <- c("rt", "amp","htt")
# remove all urls
tweets$hashtags <- gsub("(s?)(f|ht)tp(s?)://\\S+\\b", "", tweets$hashtags)
# clean data
tweets$hashtags <- tweets$hashtags %>%
removeNumbers() %>%
tolower() %>%
iconv(from = 'UTF-8', to = 'ASCII//TRANSLIT') %>% #Remove accents, foreign language items
removeWords(stopwords("SMART")) %>%
removeWords(other.words) %>%
stemDocument() %>%
stripWhitespace()
tweets$hashtags[tweets$hashtags == "NA"] <- NA
# common artifacts that remain after cleaning
other.words <- c("rt", "amp","htt")
# remove all urls
tweets$symbols <- gsub("(s?)(f|ht)tp(s?)://\\S+\\b", "", tweets$symbols)
# clean data
tweets$symbols <- tweets$symbols %>%
removeNumbers() %>%
tolower() %>%
iconv(from = 'UTF-8', to = 'ASCII//TRANSLIT') %>% #Remove accents, foreign language items
removeWords(stopwords("SMART")) %>%
removeWords(other.words) %>%
stemDocument() %>%
stripWhitespace()
tweets$symbols[tweets$symbols == "NA"] <- NA
ftm <- tweets %>%
filter(symbols == "ftm") %>%
mutate(hr = round_date(created_at, unit = "hour"))
ftm_summary <- ftm %>% count(hr)
ftm_summary %>%
ggplot() +
geom_line(aes(x = hr, y = n), alpha = .7) + # change alpha for readability
theme(axis.text.x = element_text(angle = 30)) + # make x-axis more readable
ggtitle("Tweets per Hour") +
xlab("") +
ylab("Tweets")
# read in the price data
token_prices <- read_csv("tokenprices.csv")
ftm_prices <- token_prices %>%
filter(coin_id == "fantom") %>%
mutate(hr = round_date(timestamp, unit = "hour"))
ggplot(ftm_prices) +
geom_line(aes(x = hr, y = price)) +
ggtitle("Price of FTM Over Time") +
xlab("") +
ylab("Price")
ggsave('ftmprice.png')
ftm <- ftm %>%
filter(hr >= min(ftm_prices$hr))
ftm_summary <- ftm %>% count(hr)
ftm_summary %>%
ggplot() +
geom_col(aes(x = hr, y = n)) + # change alpha for readability
theme(axis.text.x = element_text(angle = 30)) + # make x-axis more readable
ggtitle("Tweets per Hour") +
xlab("") +
ylab("Tweets")
#Merge the two tables
ftm_simple <- ftm_prices %>%
select(hr, coin_id, price) %>%
left_join(ftm_summary, by = c("hr" = "hr"))
# change nas to 0s
ftm_simple <- ftm_simple %>% mutate(n = ifelse(is.na(n), 0, n))
ggplot(ftm_simple) +
geom_point(aes(x = n, y = price)) +
ggtitle("Price vs Tweets/Hour") +
xlab("Tweets/Hour") +
ylab("Price")
tweet_volume <- ggplot(ftm_simple) +
geom_col(aes(x = hr, y = n), color = "dark blue", alpha = 0.3) +
ggtitle("Tweets/Price") +
xlab("") +
ylab("Tweets")
ftm_price <-  ggplot(ftm_simple) +
geom_line(aes(x = hr, y = price), color = "orange") +
xlab("") +
ylab("Price")
grid.arrange(tweet_volume, ftm_price)
#extract the date and the hour
ftm_cycle <- ftm_simple %>%
mutate(date = date(hr)) %>%
mutate(hour = hour(hr))
#calculate min price per day, max price per day, and the time they occured
ftm_cycle <- ftm_cycle %>%
group_by(date) %>%
mutate(min_price = min(price),
max_price = max(price),
time_min = case_when(min_price == price ~ hour),
time_max = case_when(max_price == price ~ hour),
) %>%
ungroup()
tail(ftm_cycle, 25)
min_time_summary <- ftm_cycle %>%
count(time_min)
max_time_summary <- ftm_cycle %>%
count(time_max)
min_time_plot <- ggplot(min_time_summary) +
geom_col(aes(x = time_min, y = n), fill = "dark red") +
ggtitle("Time of Day with the Lowest Prices") +
xlab("Time of Day") +
ylab("Days")
max_time_plot <- ggplot(max_time_summary) +
geom_col(aes(x = time_max, y = n), fill = "dark green") +
ggtitle("Time of Day with the Highest Prices") +
xlab("Time of Day") +
ylab("Days")
grid.arrange(min_time_plot, max_time_plot)
min_days <- ftm_cycle %>%
group_by(date) %>%
filter(time_min == 0) %>%
count(time_max) %>%
select(date)
ftm_cycle %>%
filter(date %in% min_days$date) %>%
count(time_max) %>%
ggplot() +
geom_col(aes(x = time_max, y = n)) +
ggtitle("Time of Day with the Highest Prices when the Low was at 00:00") +
xlab("Time of Day") +
ylab("Days")
max_days <- ftm_cycle %>%
group_by(date) %>%
filter(time_max == 0) %>%
count(time_min) %>%
select(date)
ftm_cycle %>%
filter(date %in% max_days$date) %>%
count(time_min) %>%
ggplot() +
geom_col(aes(x = time_min, y = n)) +
ggtitle("Time of Day with the Lowest Prices when the High was at 00:00") +
xlab("Time of Day") +
ylab("Days")
#Create a time series object from the data
ftm_simple <- ftm_simple %>%
arrange(hr) %>%
rename(tweets = n)
ftm_ts <- as.xts(ftm_simple[,3:4], order.by = ftm_simple$hr)
plot.xts(ftm_ts)
plot.xts(diff(ftm_ts[,"price"]))
plot.xts(diff(ftm_ts[,"tweets"]))
plot.xts(diff(ftm_ts))
min_max_norm <- function(x) {
(x - min(x)) / (max(x) - min(x))
}
standardized_ftm <- ftm_simple %>%
mutate(price = min_max_norm(price)) %>%
mutate(tweets = min_max_norm(tweets))
std_ts <- as.xts(standardized_ftm[,3:4], order.by = standardized_ftm$hr)
plot.xts(abs(diff(std_ts)))
ftm_ts$tweet_avg <- rollapply(ftm_ts$tweets, width = 24, FUN = mean)
plot.xts(ftm_ts$tweet_avg)
ftm_ts$price_avg <- rollapply(ftm_ts$price, width = 24, FUN = mean)
plot.xts(ftm_ts$price_avg)
ftm_ts$tweet_dev <- ftm_ts$tweets - ftm_ts$tweet_avg
plot.xts(ftm_ts$tweet_dev)
ftm_ts$price_dev <- ftm_ts$price - ftm_ts$price_avg
plot.xts(ftm_ts$price_dev)
ftm_ts$price_diff <- diff(ftm_ts$price, 1, 1)
plot.xts(ftm_ts$price_diff)
ftm_ts$price_diff_avg <- rollapply(ftm_ts$price_diff, width = 24, FUN = mean)
plot.xts(ftm_ts$price_diff_avg)
ftm_ts$price_diff_dev <- ftm_ts$price_diff - ftm_ts$price_diff_avg
plot.xts(ftm_ts$price_diff_dev)
ftm_ts_df <- as.data.frame(ftm_ts)
fit.tweets <- lm(price_diff_dev ~ tweet_dev, ftm_ts_df)
summary(fit.tweets)
ftm_ts_df$indx <- row.names(ftm_ts_df)
ftm_ts_df$indx <- gsub('^.|.$', '', ftm_ts_df$indx)
View(ftm_ts_df)
View(ftm_ts_df)
ftm_ts_df$indx <- row.names(ftm_ts_df)
ftm_ts_df$indx <- gsub('^.|.$', '', ftm_ts_df$indx)
ftm_ts_df$indx <- as.POSIXct(ftm_ts_df$indx, tryFormats = c('%Y.%m.%d.%H.%m.%OS',
'%Y.%m.%d.%H.%m',
'%Y.%m.%d.%H',
'%Y.%m.%d',
), tz = 'UTC')
ftm_ts_df$indx <- row.names(ftm_ts_df)
ftm_ts_df$indx <- gsub('^.|.$', '', ftm_ts_df$indx)
ftm_ts_df$indx <- as.POSIXct(ftm_ts_df$indx, tryFormats = c('%Y.%m.%d.%H.%m.%OS',
'%Y.%m.%d.%H.%m',
'%Y.%m.%d.%H',
'%Y.%m.%d'
), tz = 'UTC')
knitr::opts_chunk$set(echo = TRUE,
cache = FALSE)
if (!require("tidyverse")) install.packages("tidyverse")
library(tidyverse)
if (!require("tm")) install.packages("tm")
library(tm)
if (!require("ggplot2")) install.packages("ggplot2")
library(ggplot2)
if (!require("gridExtra")) install.packages("gridExtra")
library(gridExtra)
if (!require("lubridate")) install.packages("lubridate")
library(lubridate)
if (!require("xts")) install.packages("xts")
library(xts)
if (!require("zoo")) install.packages("zoo")
library(zoo)
if (!require("SnowballC")) install.packages("SnowballC")
library(forecast)
# read in csv file of tweets
rawish.data <- read.csv('influencerdata_clean.csv')
tweets <- rawish.data
tweets$created_at <- as_datetime(tweets$created_at)
tweets$quoted_created_at <- as_datetime(tweets$quoted_created_at, tz = "UTC")
tweets$retweet_created_at <- as_datetime(tweets$retweet_created_at, tz = "UTC")
tweets$account_created_at <- as_datetime(tweets$account_created_at, tz = "UTC")
# common artifacts that remain after cleaning
other.words <- c("rt", "amp","htt")
# remove all urls
tweets$hashtags <- gsub("(s?)(f|ht)tp(s?)://\\S+\\b", "", tweets$hashtags)
# clean data
tweets$hashtags <- tweets$hashtags %>%
removeNumbers() %>%
tolower() %>%
iconv(from = 'UTF-8', to = 'ASCII//TRANSLIT') %>% #Remove accents, foreign language items
removeWords(stopwords("SMART")) %>%
removeWords(other.words) %>%
stemDocument() %>%
stripWhitespace()
tweets$hashtags[tweets$hashtags == "NA"] <- NA
# common artifacts that remain after cleaning
other.words <- c("rt", "amp","htt")
# remove all urls
tweets$symbols <- gsub("(s?)(f|ht)tp(s?)://\\S+\\b", "", tweets$symbols)
# clean data
tweets$symbols <- tweets$symbols %>%
removeNumbers() %>%
tolower() %>%
iconv(from = 'UTF-8', to = 'ASCII//TRANSLIT') %>% #Remove accents, foreign language items
removeWords(stopwords("SMART")) %>%
removeWords(other.words) %>%
stemDocument() %>%
stripWhitespace()
tweets$symbols[tweets$symbols == "NA"] <- NA
ftm <- tweets %>%
filter(symbols == "ftm") %>%
mutate(hr = round_date(created_at, unit = "hour"))
ftm_summary <- ftm %>% count(hr)
ftm_summary %>%
ggplot() +
geom_line(aes(x = hr, y = n), alpha = .7) + # change alpha for readability
theme(axis.text.x = element_text(angle = 30)) + # make x-axis more readable
ggtitle("Tweets per Hour") +
xlab("") +
ylab("Tweets")
# read in the price data
token_prices <- read_csv("tokenprices.csv")
ftm_prices <- token_prices %>%
filter(coin_id == "fantom") %>%
mutate(hr = round_date(timestamp, unit = "hour"))
ggplot(ftm_prices) +
geom_line(aes(x = hr, y = price)) +
ggtitle("Price of FTM Over Time") +
xlab("") +
ylab("Price")
ggsave('ftmprice.png')
ftm <- ftm %>%
filter(hr >= min(ftm_prices$hr))
ftm_summary <- ftm %>% count(hr)
ftm_summary %>%
ggplot() +
geom_col(aes(x = hr, y = n)) + # change alpha for readability
theme(axis.text.x = element_text(angle = 30)) + # make x-axis more readable
ggtitle("Tweets per Hour") +
xlab("") +
ylab("Tweets")
#Merge the two tables
ftm_simple <- ftm_prices %>%
select(hr, coin_id, price) %>%
left_join(ftm_summary, by = c("hr" = "hr"))
# change nas to 0s
ftm_simple <- ftm_simple %>% mutate(n = ifelse(is.na(n), 0, n))
ggplot(ftm_simple) +
geom_point(aes(x = n, y = price)) +
ggtitle("Price vs Tweets/Hour") +
xlab("Tweets/Hour") +
ylab("Price")
tweet_volume <- ggplot(ftm_simple) +
geom_col(aes(x = hr, y = n), color = "dark blue", alpha = 0.3) +
ggtitle("Tweets/Price") +
xlab("") +
ylab("Tweets")
ftm_price <-  ggplot(ftm_simple) +
geom_line(aes(x = hr, y = price), color = "orange") +
xlab("") +
ylab("Price")
grid.arrange(tweet_volume, ftm_price)
#extract the date and the hour
ftm_cycle <- ftm_simple %>%
mutate(date = date(hr)) %>%
mutate(hour = hour(hr))
#calculate min price per day, max price per day, and the time they occured
ftm_cycle <- ftm_cycle %>%
group_by(date) %>%
mutate(min_price = min(price),
max_price = max(price),
time_min = case_when(min_price == price ~ hour),
time_max = case_when(max_price == price ~ hour),
) %>%
ungroup()
tail(ftm_cycle, 25)
min_time_summary <- ftm_cycle %>%
count(time_min)
max_time_summary <- ftm_cycle %>%
count(time_max)
min_time_plot <- ggplot(min_time_summary) +
geom_col(aes(x = time_min, y = n), fill = "dark red") +
ggtitle("Time of Day with the Lowest Prices") +
xlab("Time of Day") +
ylab("Days")
max_time_plot <- ggplot(max_time_summary) +
geom_col(aes(x = time_max, y = n), fill = "dark green") +
ggtitle("Time of Day with the Highest Prices") +
xlab("Time of Day") +
ylab("Days")
grid.arrange(min_time_plot, max_time_plot)
min_days <- ftm_cycle %>%
group_by(date) %>%
filter(time_min == 0) %>%
count(time_max) %>%
select(date)
ftm_cycle %>%
filter(date %in% min_days$date) %>%
count(time_max) %>%
ggplot() +
geom_col(aes(x = time_max, y = n)) +
ggtitle("Time of Day with the Highest Prices when the Low was at 00:00") +
xlab("Time of Day") +
ylab("Days")
max_days <- ftm_cycle %>%
group_by(date) %>%
filter(time_max == 0) %>%
count(time_min) %>%
select(date)
ftm_cycle %>%
filter(date %in% max_days$date) %>%
count(time_min) %>%
ggplot() +
geom_col(aes(x = time_min, y = n)) +
ggtitle("Time of Day with the Lowest Prices when the High was at 00:00") +
xlab("Time of Day") +
ylab("Days")
#Create a time series object from the data
ftm_simple <- ftm_simple %>%
arrange(hr) %>%
rename(tweets = n)
ftm_ts <- as.xts(ftm_simple[,3:4], order.by = ftm_simple$hr)
plot.xts(ftm_ts)
plot.xts(diff(ftm_ts[,"price"]))
plot.xts(diff(ftm_ts[,"tweets"]))
plot.xts(diff(ftm_ts))
min_max_norm <- function(x) {
(x - min(x)) / (max(x) - min(x))
}
standardized_ftm <- ftm_simple %>%
mutate(price = min_max_norm(price)) %>%
mutate(tweets = min_max_norm(tweets))
std_ts <- as.xts(standardized_ftm[,3:4], order.by = standardized_ftm$hr)
plot.xts(abs(diff(std_ts)))
ftm_ts$tweet_avg <- rollapply(ftm_ts$tweets, width = 24, FUN = mean)
plot.xts(ftm_ts$tweet_avg)
ftm_ts$price_avg <- rollapply(ftm_ts$price, width = 24, FUN = mean)
plot.xts(ftm_ts$price_avg)
ftm_ts$tweet_dev <- ftm_ts$tweets - ftm_ts$tweet_avg
plot.xts(ftm_ts$tweet_dev)
ftm_ts$price_dev <- ftm_ts$price - ftm_ts$price_avg
plot.xts(ftm_ts$price_dev)
ftm_ts$price_diff <- diff(ftm_ts$price, 1, 1)
plot.xts(ftm_ts$price_diff)
ftm_ts$price_diff_avg <- rollapply(ftm_ts$price_diff, width = 24, FUN = mean)
plot.xts(ftm_ts$price_diff_avg)
ftm_ts$price_diff_dev <- ftm_ts$price_diff - ftm_ts$price_diff_avg
plot.xts(ftm_ts$price_diff_dev)
ftm_ts_df <- as.data.frame(ftm_ts)
fit.tweets <- lm(price_diff_dev ~ tweet_dev, ftm_ts_df)
summary(fit.tweets)
